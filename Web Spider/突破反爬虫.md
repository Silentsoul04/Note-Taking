## USE_AGENT

修改 settings 文件中

```python
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0'
```

***

## Selenium

Selenium 框架底层使用JavaScript模拟真实用户对浏览器进行操作。以Chrome浏览器为例。

### 通过 Chromedriver 自动打开Chrome

```python
# -*- coding: utf-8 -*-
import scrapy
from selenium import webdriver

class SinaSpider(scrapy.Spider):
    ...

    def start_requests(self):
        browser = webdriver.Chrome(executable_path=r'C:\GitHub\spiders\NewsSpider\tools\chromedriver.exe')
        browser.get('https://xxx')
        return [scrapy.Request(url=self.start_urls[0], dont_filter=True)]
```

### 手动启动 Chrome

有些网站会阻止，通过Chromedriver自动打开chrome浏览器的访问。那么，现在通过手动启动chrome浏览器。

通过cmd打开chrome

```powershell
# 在chrome 所在文件夹执行
chrome.exe --remote-debugging-port=9222
```

**NOTE**:启动之前，必须确保所有chrome实例已经关闭

此时，通过`127.0.0.1:9222/json`，能过正确访问，就表示启动成功。

```python
# -*- coding: utf-8 -*-
import scrapy
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

class SinaSpider(scrapy.Spider):
    ...

    def start_requests(self):
        chrome_option = Options()
        chrome_option.add_argument('--disable-extensions')
        chrome_option.add_experimental_option('debuggerAddress', '127.0.0.1:9222')
        browser = webdriver.Chrome(
            executable_path=r'C:\GitHub\spiders\NewsSpider\tools\chromedriver.exe',
            chrome_options=chrome_option)
        browser.get('https://xxx')
        
        return [scrapy.Request(url=self.start_urls[0], dont_filter=True)]
```

### 模拟登录

模拟登录

```python
# -*- coding: utf-8 -*-
import scrapy
import time
from selenium import webdriver

class SinaSpider(scrapy.Spider):
    ...

    def start_requests(self):
        browser = webdriver.Chrome(executable_path=r'C:\GitHub\spiders\NewsSpider\tools\chromedriver.exe')
        browser.get('https://xxx')	# 登录页面URL
        browser.find_element_by_css_selector("xxx").send_keys(Keys.CONTROL + "a")
        browser.find_element_by_css_selector("xxx").send_keys("xxx")
        browser.find_element_by_css_selector("xxx").send_keys(Keys.CONTROL + "a")
        browser.find_element_by_css_selector("xxx").send_keys("xxx")
        browser.find_element_by_css_selector("xxx").click()
        time.sleep(10)

        return [scrapy.Request(url=self.start_urls[0], dont_filter=True)]
```

### 存储 cookies

登录成功后，存储登录后的cookies，从而实现之后访问时，自动登录

```python
# -*- coding: utf-8 -*-
import scrapy
import pickle
import time
from selenium import webdriver

class SinaSpider(scrapy.Spider):
    ...

    def start_requests(self):
        ...
		cookies = browser.get_cookies()
        cookie_dict = {}
        for cookie in cookies:
			f = open(r'xxx' + cookie['name'] + '.sina', 'wb')
            pickle.dump(cookie, f)
        f.close()
        cookie_dict[cookie['name']] = cookie['value']
        browser.close()
```

同时，需要将 settings 文件中，设置：

```python
COOKIES_ENABLED = True
```

