一个基本的爬虫通常分为数据采集（网页下载）、数据处理（网页解析）和数据存储（将有用的信息持久化）三个部分的内容，当然更为高级的爬虫在数据采集和处理时会使用并发编程或分布式技术，这就需要有调度器（安排线程或进程执行对应的任务）、后台管理程序（监控爬虫的工作状态以及检查数据抓取的结果）等的参与。

![](https://note-taking-1258869021.cos.ap-beijing.myqcloud.com/Web%20Spider/crawler-workflow.png)

爬虫的设计思路：

1. 确定需要爬取的URL
2. 获取对应的HTML页面
3. 提取HTML页面数据
   * 如果是目标数据，就储存
   * 如果是页面里包含其他URL，则重复2、3步骤

HTML页面通常由三部分构成，分别是

* 用来承载内容的Tag（标签）
* 负责渲染页面的CSS（层叠样式表）
* 控制交互式行为的JavaScript。

***

## URL

HTTP请求的处理库：urllib、urllib2、requests

* 处理相对链接
* URL 去重

## 数据采集和解析

爬虫开发相关技术的清单以及这些技术涉及到的标准库和第三方库：

* 下载数据 - urllib / requests / aiohttp。
* 解析数据 - re / lxml / beautifulsoup4 / pyquery。
* 缓存和持久化 - pymysql / sqlalchemy / peewee/ redis / pymongo。
* 生成数字签名 - hashlib。
* 序列化和压缩 - pickle / json / zlib。
* 调度器 - 多进程（multiprocessing） / 多线程（threading）。

具体的处理就是使用某种描述性语言来给我们需要提取的数据定义一个匹配规则，符合这个规则的数据就会被匹配。



### 页面解析

几种解析方式的比较

| 解析方式     | 对应的模块 | 速度   | 使用难度 | 备注                                    |
| ------------ | ---------- | ------ | -------- | --------------------------------------- |
| 正则表达式   | re         | 快     | 困难     | 常用正则表达式 在线正则表达式测试       |
| XPath 选择器 | lxml       | 快     | 一般     | 需要安装C语言依赖库 唯一支持XML的解析器 |
| CSS 选择器   | pyquery    | 不确定 | 简单     |                                         |

BeautifulSoup可选的解析器包括：

* Python 标准库（html.parser）
* lxml HTML 解析器
* lxml XML 解析器
* html5lib

其中，lxml解析最快。

## 爬虫 - 反爬虫 - 反反爬虫

通用的反爬虫技术：User-Agent、验证码、动态数据加载、加密数据。

* 设置代理
* 限制下载速度

* 采集动态HTML以及验证码的处理

通用的动态页面采集：Selenium + PhantomJS(无界面)：模拟真实浏览器加载js、ajax等非静态页面数据
对于验证码的处理可以使用 Tesseract库，这是一个Google的机器学习库，基于机器图像识别系统，可以处理简单的验证码。但是复杂的验证码可以通过手动输入或者找专门的打码平台来处理。

***

## 相关工具

* Firebug
* Chrome Developer Tools
* POSTMAN：功能强大的网页调试与RESTful请求工具
* HTTPie：命令行HTTP客户端
* BuiltWith：识别网站所用技术的工具
* python-whois：查询网站所有者的工具
* robotparser：解析robots.txt的工具

***

参考：

[网络爬虫和相关工具](https://github.com/jackfrued/Python-100-Days/blob/master/Day66-75/66.%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7.md)，骆昊