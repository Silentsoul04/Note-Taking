## 目的

获取豆瓣电影 Top 250，网址：

```text
https://movie.douban.com/top250
```

并循环5次，计算平均计算所需时间。

## 方法1：使用 urllib.request

```python
import urllib.request
import ssl
from lxml import etree

url = 'https://movie.douban.com/top250'
context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_1)

def fetch_page(url):
    response = urllib.request.urlopen(url, context=context)
    return response

def parse(url):
    response = fetch_page(url)
    page = response.read()
    html = etree.HTML(page)

    xpath_movie = '//*[@id="content"]/div/div[1]/ol/li'     # 电影栏    
    xpath_pages = '//*[@id="content"]/div/div[1]/div[2]/a'  # 底端页码
    xpath_title = './/span[@class="title"]'                 # 电影名称

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):   # 获取每一栏电影信息
        result.append(element_movie)

    for p in pages:                                 # 获取每个页码的url
        fetch_list.append(url + p.get('href'))          

    for url in fetch_list:	# 遍历页码url,获取每页电影信息
        response = fetch_page(url)
        page = response.read()
        html = etree.HTML(page)
        for element_movie in html.xpath(xpath_movie):
            result.append(element_movie)

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text    # NOTE：此处用的是find(),虽然有两个名称，但是提取第一个         
        print(i, title)

def main():
    from time import time
    start = time()    
    for i in range(5):
        parse(url)
    end = time()
    print ('Cost {} seconds'.format((end - start) / 5))

if __name__ == '__main__':
    main()
```

```
Cost 1.3677135944366454 seconds
```

## 方法2：使用 requests

```python
import requests
from lxml import etree

url = 'https://movie.douban.com/top250'

def fetch_page(url):
    response = requests.get(url)
    return response

def parse(url):
    response = fetch_page(url)
    page = response.content
    html = etree.HTML(page)

    xpath_movie = '//*[@id="content"]/div/div[1]/ol/li'     # 电影栏    
    xpath_pages = '//*[@id="content"]/div/div[1]/div[2]/a'  # 底端页码
    xpath_title = './/span[@class="title"]'                 # 电影名称

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):   # 获取每一栏电影信息
        result.append(element_movie)

    for p in pages:                                 # 获取每个页码的url
        fetch_list.append(url + p.get('href'))          

    for url in fetch_list:  # 遍历页码url,获取每页电影信息
        response = fetch_page(url)
        page = response.content
        html = etree.HTML(page)
        for element_movie in html.xpath(xpath_movie):
            result.append(element_movie)

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text    # NOTE：此处用的是find(),虽然有两个名称，但是提取第一个         
        print(i, title)

def main():
    from time import time
    start = time()    
    for i in range(5):
        parse(url)
    end = time()
    print ('Cost {} seconds'.format((end - start) / 5))

if __name__ == '__main__':
    main()
```

```text
Cost 1.3092393398284912 seconds
```

## 方法3：使用 re

```python
import requests
import re

url = 'https://movie.douban.com/top250'

def fetch_page(url):
    response = requests.get(url)
    return response

def parse(url):
    response = fetch_page(url)
    page = response.content

    fetch_list = set()
    result = []

    for title in re.findall(rb'<a href=.*\s.*<span class="title">(.*)</span>', page):
        result.append(title)
    
    for postfix in re.findall(rb'<a href="(\?start=.*?)"', page):
        fetch_list.add(url + postfix.decode())
    
    for url in fetch_list:
        response = fetch_page(url)
        page = response.content
        for title in re.findall(rb'<a href=.*\s.*<span class="title">(.*)</span>', page):
            result.append(title)

    for i, title in enumerate(result, 1):
        title = title.decode()
        print(i, title)

def main():
    from time import time
    start = time()    
    for i in range(5):
        parse(url)
    end = time()
    print ('Cost {} seconds'.format((end - start) / 5))

if __name__ == '__main__':
    main()
```

```text
Cost 1.278094434738159 seconds
```

## 方法4：使用 多线程

```python
import requests
from lxml import etree
from threading import Thread

url = 'https://movie.douban.com/top250'

def fetch_page(url):
    response = requests.get(url)
    return response

def parse(url):
    response = fetch_page(url)
    page = response.content
    html = etree.HTML(page)

    xpath_movie = '//*[@id="content"]/div/div[1]/ol/li'     # 电影栏    
    xpath_pages = '//*[@id="content"]/div/div[1]/div[2]/a'  # 底端页码
    xpath_title = './/span[@class="title"]'                 # 电影名称

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):   # 获取每一栏电影信息
        result.append(element_movie)

    for p in pages:                                 # 获取每个页码的url
        fetch_list.append(url + p.get('href'))          

    def fetch_content(url):  # 遍历页码url,获取每页电影信息
        response = fetch_page(url)
        page = response.content
        html = etree.HTML(page)
        for element_movie in html.xpath(xpath_movie):
            result.append(element_movie)
    
    threads = []
    for url in fetch_list:
        t = Thread(target=fetch_content, args=[url])
        t.start()
        threads.append(t)

    for t in threads:
        t.join()

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text    # NOTE：此处用的是find(),虽然有两个名称，但是提取第一个         
        print(i, title)

def main():
    from time import time
    start = time()    
    for i in range(5):
        parse(url)
    end = time()
    print ('Cost {} seconds'.format((end - start) / 5))

if __name__ == '__main__':
    main()
```

```text
Cost 0.05768890380859375 seconds
```

## 方法5：使用 多进程

```python
import requests
from lxml import etree
from concurrent.futures import ProcessPoolExecutor

url = 'https://movie.douban.com/top250'

def fetch_page(url):
    response = requests.get(url)
    return response

def fetch_content(url):  # 遍历页码url,获取每页电影信息
        response = fetch_page(url)
        page = response.content
        return page
        
def parse(url):
    response = fetch_page(url)
    page = response.content
    html = etree.HTML(page)

    xpath_movie = '//*[@id="content"]/div/div[1]/ol/li'     # 电影栏    
    xpath_pages = '//*[@id="content"]/div/div[1]/div[2]/a'  # 底端页码
    xpath_title = './/span[@class="title"]'                 # 电影名称

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):   # 获取每一栏电影信息
        result.append(element_movie)

    for p in pages:                                 # 获取每个页码的url
        fetch_list.append(url + p.get('href'))          
    
    with ProcessPoolExecutor(max_workers=4) as executor:
        for url in executor.map(fetch_content, fetch_list):
            html = etree.HTML(page)
            for element_movie in html.xpath(xpath_movie):
                result.append(element_movie)


    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text    # NOTE：此处用的是find(),虽然有两个名称，但是提取第一个         
        print(i, title)

def main():
    from time import time
    start = time()    
    for i in range(5):
        parse(url)
    end = time()
    print ('Cost {} seconds'.format((end - start) / 5))

if __name__ == '__main__':
    main()
```

```text
Cost 2.3680461406707765 seconds
```

## 方法6：使用 asyncio、aiohttp

```python
from lxml import etree
import asyncio
import aiohttp

url = 'https://movie.douban.com/top250'

async def fetch_content(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.text()

async def parse(url):
    page = await fetch_content(url)
    html = etree.HTML(page)

    xpath_movie = '//*[@id="content"]/div/div[1]/ol/li'
    xpath_title = './/span[@class="title"]'
    xpath_pages = '//*[@id="content"]/div/div[1]/div[2]/a'

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):
        result.append(element_movie)

    for p in pages:
        fetch_list.append(url + p.get('href'))

    tasks = [fetch_content(url) for url in fetch_list]
    pages = await asyncio.gather(*tasks)

    for page in pages:
        html = etree.HTML(page)
        for element_movie in html.xpath(xpath_movie):
            result.append(element_movie)

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text
        print(i, title)


def main():
    from time import time
    loop = asyncio.get_event_loop()    
    start = time()    
    for i in range(5):
        loop.run_until_complete(parse(url))
    end = time()
    print ('Cost {} seconds'.format((end - start) / 5))
    loop.close()


if __name__ == '__main__':
    main()
```

```text
Cost 0.5916292190551757 seconds
```

## 总结

| 方案                        | 耗时（s）           |
| --------------------------- | ------------------- |
| urllib.request              | 1.3677135944366454  |
| request 替代 urllib.request | 1.3092393398284912  |
| re 解析替代 lxml解析        | 1.278094434738159   |
| 多线程                      | 0.05768890380859375 |
| 多进程                      | 2.3680461406707765  |
| 异步                        | 0.5916292190551757  |

request虽然比用urllib快，但是总体来说，他们基本还是处于同一水平线的，程序并没有快很多，这一点的差距或许是requests对请求做了优化导致的。

re 解析替代 lxml解析，也是程序运行速度加快。与此同时，依赖原生正则表达式的程序会更加难以维护。其次，这种网络应用通常瓶颈都在IO层面，解决等待读写的问题比提高文本解析速度来的更有性价比！

多线程有效的解决了阻塞等待的问题，速度明显加快。

python的多线程也受制于GIL，尝试使用多进程。但是，多进程甚至还不如多线程。多进程带来的优点（cpu处理）并没有得到体现，反而创建和调度进程带来的开销要远超出它的正面效应，拖了一把后腿。

多进程和多线程除了创建的开销大之外还有一个难以根治的缺陷，就是处理进程之间或线程之间的协作问题，因为是依赖多进程和多线程的程序在不加锁的情况下通常是不可控的，而协程则可以完美地解决协作问题，由用户来决定协程之间的调度。

在实际使用中，应该根据使用场景来挑选最合适的应用方案，影响程序效率的因素有很多，以上不同的异步方式在不同的场景下也会有不一样的表现，不要抱死在一个大树上，该用同步的地方用同步，该用异步的地方异步，这样才能构建出更加灵活的网络应用。

Python因为有GIL（全局解释锁）这玩意，不可能有真正的多线程的存在，因此很多情况下都会用multiprocessing实现并发，而且在Python中应用多线程还要注意关键地方的同步，不太方便，用协程代替多线程和多进程是一个很好的选择，因为它吸引人的特性：主动调用/退出，状态保存，避免cpu上下文切换等…

***

摘抄：

https://zhuanlan.zhihu.com/p/25228075