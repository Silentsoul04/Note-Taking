## 获取内链、外链

```python
# _*_coding:utf-8_*_

from urllib.request import urlopen
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import re
import datetime
import random

pages = set()
random.seed(datetime.datetime.now())

# 获取内链
def get_internal_links(bsObj, include_url):
    include_url = "{}://{}".format(urlparse(include_url).scheme, urlparse(include_url).netloc)
    internal_links = []

    for link in bsObj.findAll("a",
                        href=re.compile("^(/|.*"+include_url+")")):
        if link.attrs['href'] is not None:
            if link.attrs['href'] not in internal_links:
                if(link.attrs['href'].startswith("/")):
                    internal_links.append(include_url + link.attrs['href'])
    return internal_links

# 获取外链
def get_external_links(bsObj, exclude_url):
    external_links = []
    for link in bsObj.findAll("a",
                        href = re.compile("^(http|www)((?!"+exclude_url+").)*$")):
        if link.attrs['href'] is not None:
            if link.attrs['href'] not in external_links:
                external_links.append(link.attrs['href'])
    return external_links

def get_random_external_link(starting_page):
    html = urlopen(starting_page)
    bsObj = BeautifulSoup(html, "html.parser")
    external_links = get_external_links(bsObj, urlparse(starting_page).netloc)
    if len(external_links) == 0:
        print("no external links, looking around the site for one")
        domain = "{}://{}".format(urlparse(starting_page).scheme, urlparse(starting_page).netloc)
        internal_links = get_internal_links(bsObj, domain)
        return get_random_external_link(internal_links[random.randint(0, len(internal_links)-1)])
    else:
        return external_links[random.randint(0, len(external_links)-1)]

def follow_external_only(starting_site):
    external_link = get_random_external_link(starting_site)
    print("random external link is" + external_link)
    follow_external_only(external_link)

follow_external_only("https://en.wikipedia.org/wiki/Main_Page")
```

***

## API 的使用

获取wiki python词条编辑者的 IP 地址，并由 [freegeoip](http://freegeoip.net/json/) 的 API 解析，从而获得编辑者的国籍。

### 思路

![思路](https://note-taking-1258869021.cos.ap-beijing.myqcloud.com/Web%20Spider/freegeoip.jpg)

***

### 实现

```python
# _*_coding:utf-8_*_

from urllib.request import urlopen
from urllib.request import HTTPError
from bs4 import BeautifulSoup
import datetime
import random
import re
import json

random.seed(datetime.datetime.now())
# 获取内链
def get_links(article_url):
    html = urlopen("http://en.wikipedia.org" + article_url)
    bsObj = BeautifulSoup(html, "html.parser")
    return bsObj.find("div", {"id":"bodyContent"}).findAll("a",
                        href=re.compile("^(/wiki/)((?!:).)*$"))
# 获取编辑者 IP
def get_history_ip(page_url):
    page_url = page_url.replace("/wiki/", "")
    history_url = "{}{}{}".format("http://en.wikipedia.org/w/index.php?title=",
                                    page_url, "&action=history")
    print("history url is:"+history_url)
    html = urlopen(history_url)
    bsObj = BeautifulSoup(html, "html.parser")
    ip_addresses = bsObj.findAll("a", {"class":"mw-anonuserlink"})
    address_list = set()
    for ip_address in ip_addresses:
        address_list.add(ip_address.get_text())
    return address_list
# 由 freegeoip 的 API解析
def get_country(ip_address):
    try:
        response = urlopen("http://freegeoip.net/json/"+ip_address).read().decode('utf-8')
    except HTTPError:
        return None
    response_json = json.loads(response)
    return response_json["region_name"]

links = get_links("/wiki/Python_(programming_language)")

while (len(links) > 0):
    for link in links:
        print("--------------------")
        history_ips = get_history_ip(link.attrs["href"])
        for history_ip in history_ips:
            country = get_country(history_ip)
            if country is not None:
                print(history_ip+ " is from " + country)
    new_link = links[random.randint(0, len(links)-1).attrs["href"]]
    links = get_links(new_link)
```

***

稍麻烦的地方在：
`get_links()` 与 `get_hitory_ip()` 之间的 `URL` 的传递。

* 首先，`get_links()` 传入 `/wiki/Python_(programming_language)`；
* 然后，`get_links()` 输出该页爬取的内链列表；
* 接着，`get_hitory_ip()` 传入 `get_links()` 刚刚计算的列表；
* 然而，内链连接以 `/wiki/` 开头，需要整理成编辑者页面的 URL；
* 因此，需要将传入列表的 `URL` 进行整理，即采用 `replace()` 和 `format()` 函数。

最后，交于 freegeoip 进行处理，计算出 ip 对应的地区。freegeoip 返回的结果是 `JSON`，因此需要进行解析。

***

输出结果：

```txt
--------------------
history url is:http://en.wikipedia.org/w/index.php?title=Programming_paradigm&action=history
31.203.136.191 is from Al Asimah
168.216.130.133 is from West Virginia
42.111.56.168 is from National Capital Territory of Delhi
218.17.157.55 is from Guangdong
2605:6000:ec0f:c800:edfd:179f:b648:b4b9 is from Texas
39.36.182.41 is from Punjab
110.55.67.15 is from National Capital Region
223.230.96.108 is from Telangana
92.115.222.143 is from Chișinău Municipality
192.117.105.47 is from Tel Aviv
2a02:c7d:a492:f200:e126:2b36:53ca:513a is from England
223.104.186.241 is from Shandong
193.80.242.220 is from Lower Austria
68.151.180.83 is from Alberta
197.255.127.246 is from Greater Accra Region
--------------------
history url is:http://en.wikipedia.org/w/index.php?title=Object-oriented_programming&action=history
93.136.125.208 is from City of Zagreb
119.152.87.84 is from Punjab
103.16.68.215 is from Karnataka
2605:a601:474:600:2088:fbde:7512:53b2 is from Missouri
103.74.23.139 is from 
81.209.9.65 is from Ostrobothnia
172.56.32.238 is from California
217.225.8.24 is from 
160.53.250.90 is from Geneva
202.70.74.215 is from Central Region
61.0.82.193 is from Uttar Pradesh
47.184.120.251 is from Texas
--------------------
history url is:http://en.wikipedia.org/w/index.php?title=Imperative_programming&action=history
194.181.240.192 is from Lower Silesia
176.60.44.112 is from 
117.136.79.80 is from Guangdong
```

***

## 下载指定类型文件

下载 [pythonscraping](http://www.pythonscraping.com/) 主页的图片。

### 思路

* 网页 `HTML` 文件中，具有 `SRC` 属性文件有些是 `JS` 文件，因此需要剔除；
* 同时，需要剔除外链。

```python
if base_url not in url or ".js" in url:
    return None
```

* `get_absolute_url()` 函数的标准化功能，配合 g`et_download_paht()` 函数，可以创建整齐的下载目录；
* 最后，由 `urlretrieve()` 函数下载清洗过的 `URL`。

***

### 实现

```python
# _*_ coding:utf-8_*_

import os
from urllib.request import urlretrieve, urlopen
from bs4 import BeautifulSoup

download_directory = "downloaded"
base_url = "http://pythonscraping.com"

# 清理和标准化 URL
def get_absolute_url(base_url, source):
    if source.startswith("http://www."):
        url = "http://"+source[11:]
    elif source.startswith("http://"):
        url = source
    elif source.startswith("www."):
        url = "http://"+source
    else:
        url = "{}/{}".format(base_url, source)
    if base_url not in url or ".js" in url:
        return None
    return url
# 建立下载目录
def get_download_path(base_url, absolute_url, download_directory):
    path = absolute_url.replace("www", "")
    path = path.replace(base_url, "")
    path = download_directory+path
    directory = os.path.dirname(path)

    if not os.path.exists(directory):
        os.makedirs(directory)
    return path

html = urlopen("http://www.pythonscraping.com")
bsObj = BeautifulSoup(html, "html.parser")
download_list = bsObj.findAll(src=True)

for download in download_list:
    print("未整理 URL："+download["src"])
    file_url = get_absolute_url(base_url, download["src"])
    if file_url is not None:
        print("已整理并下载 URL："+file_url)
        urlretrieve(file_url, get_download_path(base_url, file_url, download_directory))

```

***

* 输出结果：

可以直观看出，`JS` 文件已被过滤，以及哪些文件被下载。

```shell
未整理 URL：http://www.pythonscraping.com/misc/jquery.js?v=1.4.4
未整理 URL：http://www.pythonscraping.com/misc/jquery.once.js?v=1.2
未整理 URL：http://www.pythonscraping.com/misc/drupal.js?os2esm
未整理 URL：http://www.pythonscraping.com/sites/all/themes/skeletontheme/js/jquery.mobilemenu.js?os2esm
未整理 URL：http://www.pythonscraping.com/sites/all/modules/google_analytics/googleanalytics.js?os2esm
未整理 URL：http://www.pythonscraping.com/sites/default/files/lrg_0.jpg
已整理并下载 URL：http://pythonscraping.com/sites/default/files/lrg_0.jpg
未整理 URL：http://www.oreilly.com/authors/widgets/669.html
未整理 URL：http://pythonscraping.com/img/lrg%20(1).jpg
已整理并下载 URL：http://pythonscraping.com/img/lrg%20(1).jpg
```

***

## 获取数据并存储在 `CSV`

获取 wiki [文本编辑器对比](https://en.wikipedia.org/wiki/Comparison_of_text_editors)表格数据并存储在 `CSV` 文件中。以存储第一个展示表格为例。

示例表格

![文本编辑器对比](https://note-taking-1258869021.cos.ap-beijing.myqcloud.com/Web%20Spider/List%20of%20text%20editors.png)

## 思路

* 首先，由 ```bsObj.findAll("table", {"class":"wikitable"})[0]``` 获取页面中第一个表格，即上图所示表格；
* 然后，```table.findAll("tr")``` 构建一个列表，列表以上表中的一行为一个元素进行存储；
* 最后，经由 ```for``` 循环，将上述列表的每一个元素进行解析，并写入 `CSV` 文件。

## 实现

```python
# _ _coding:utf-8_ _

import csv
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen("http://en.wikipedia.org/wiki/Comparison_of_text_editors")
bsObj = BeautifulSoup(html, "html.parser")

table = bsObj.findAll("table", {"class":"wikitable"})[0]  # 获得页面中第一个表格
rows = table.findAll("tr")  # 以一行为一个列表元素，存储整个表格

# windows 路径要输入完整，若不适用 r""，这路径中使用 \\ 转义字符
csv_file = open(r"C:\GitHub\crawler_tests\files\editors.csv", 'wt', newline='', encoding='utf-8')
writer = csv.writer(csv_file)

try:
    for row in rows:
        csv_row = []
        # 传递列表，那么 find_all() 会逐个找到在列表中元素
        for cell in row.findAll(['td', 'th']):
            csv_row.append(cell.get_text())
        writer.writerow(csv_row)
finally:
    csv_file.close()

```

***

`open()` 函数

创建 `CSV` 时，如果在 Windows 平台中，以下两个方式都是可以成功创建：

```python
csv_file = open(r"C:\GitHub\crawler_tests\files\editors.csv", 'wt', newline='', encoding='utf-8')
csv_file = open("C:\\GitHub\\crawler_tests\\files\\editors.csv", 'wt', newline='', encoding='utf-8')
```

如果采用相对路径的方式，如下所示代码，无论是否有前缀 `r`，或者转义字符，都无法成功创建，并返回 `IOError`。

```python
csv_file = open("..\files\editors.csv", 'wt', newline='', encoding='utf-8')
```

***

## 获取数据并存储在 `MySQL`

获取 wiki 中，Kevin_Bacon 的内链，并存储在 MySQL 中。

```python
# _*_coding:utf-8_*_

from urllib.request import urlopen
from bs4 import BeautifulSoup
import re
import datetime
import random
import pymysql

connect = pymysql.connect(
    host='127.0.0.1',
    port=3306,
    user='root',
    password='qixt',
    db='mysql',
    charset='utf8'
)
cursor = connect.cursor()
cursor.execute("USE scraping")

random.seed(datetime.datetime.now())

def store(title, content):
    cursor.execute("INSERT INTO pages (title, content) VALUES (\"%s\",\"%s\")", (title, content))
    cursor.connection.commit()

def get_links(article_url):
    html = urlopen("http://en.wikipedia.org"+article_url)
    bsObj = BeautifulSoup(html, "html.parser")
    title = bsObj.find("h1").get_text()
    content = bsObj.find("div", {"id":"mw-content-text"}).find("p").get_text()
    store(title, content)
    return bsObj.find("div", {"id":"bodyContent"}).findAll("a", href=re.compile("^(/wiki/)((?!:).)*$"))

links = get_links("/wiki/Kevin_Bacon")
try:
    while len(links) > 0:
        new_article = links[random.randint(0, len(links)-1)].attrs["href"]
        print("new_article:"+new_article)
        links = get_links(new_article)
finally:
    cursor.close()
    connect.close()
```

***

参考：

《Python 网络数据采集》